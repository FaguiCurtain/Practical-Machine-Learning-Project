---
title: "Practical Learning Project"
output: html_document
---

Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

We are given 2 sets, a training set of 19622 obs of 160 variables, and a testing set with only 20 obs of 160 variables.

most rows have a lot of NAs for a lot of variables. what happens, is that at some intervals, the devices were making a "full checkup" and collecting more info (maybe average of values during an interval ?) theres 406 such complete observations in the set. Outside those 406 obs, a lot of variables are NA.
For this study, i removed the variables with NA, and/or invalid values such as "". some variables had incorrectly their format as "factor" and i converted it to numeric.
I'm ending up with "only"" 84 variables.


```{r data load,cache=TRUE,echo=FALSE}
raw_training_data <- read.csv("pml-training.csv")
testing  <- read.csv("pml-testing.csv")
a <- raw_training_data
u <- a$new_window=="yes"
b <- a[u,] #'data.frame':        406 obs. of  160 variables:

v <- is.na(a)
r <- colSums(v)
toomanyNA <- (r>15000)

aa <- a[,!toomanyNA] #exclude variables taken only at times

# kurtosis_yaw_belt invalid (column 14)
# skewness_yaw_belt invalid (column 17)
# amplitude_yaw_belt invalid (column 20)
# kurtosis_yaw_dumbbell invalid (column 54)
# skewness_yaw_dumbbell invalid (column 57)
# amplitude_yaw_dumbbell invalid (column 60)
# kurtosis_yaw_forearm invalid (column 76)
# skewness_yaw_forearm invalid (column 79)
# skewness_yaw_forearm invalid (column 82)

aa <- aa[,-c(14,17,20,54,57,60,76,79,82)]

#convert variables to numeric
aa$kurtosis_roll_belt  <- as.numeric(aa$kurtosis_roll_belt)
aa$kurtosis_picth_belt <- as.numeric(aa$kurtosis_picth_belt)
aa$skewness_roll_belt <- as.numeric(aa$skewness_roll_belt)
aa$skewness_roll_belt.1 <- as.numeric(aa$skewness_roll_belt.1)
aa$max_yaw_belt <- as.numeric(aa$max_yaw_belt)
aa$min_yaw_belt <- as.numeric(aa$min_yaw_belt)
aa$kurtosis_roll_arm <- as.numeric(aa$kurtosis_roll_arm)
aa$kurtosis_picth_arm <- as.numeric(aa$kurtosis_picth_arm)
aa$kurtosis_yaw_arm <- as.numeric(aa$kurtosis_yaw_arm)
aa$skewness_roll_arm <- as.numeric(aa$skewness_roll_arm)
aa$skewness_pitch_arm <- as.numeric(aa$skewness_pitch_arm)
aa$skewness_yaw_arm <- as.numeric(aa$skewness_yaw_arm)
aa$kurtosis_roll_dumbbell <- as.numeric(aa$kurtosis_roll_dumbbell)
aa$kurtosis_picth_dumbbell <- as.numeric(aa$kurtosis_picth_dumbbell)
aa$skewness_roll_dumbbell <- as.numeric(aa$skewness_roll_dumbbell)
aa$skewness_pitch_dumbbell <- as.numeric(aa$skewness_pitch_dumbbell)
aa$max_yaw_dumbbell <- as.numeric(aa$max_yaw_dumbbell)
aa$min_yaw_dumbbell <- as.numeric(aa$min_yaw_dumbbell)
aa$total_accel_dumbbell <- as.numeric(aa$total_accel_dumbbell)
aa$kurtosis_roll_forearm <- as.numeric(aa$kurtosis_roll_forearm)
aa$kurtosis_picth_forearm <- as.numeric(aa$kurtosis_picth_forearm)
aa$skewness_roll_forearm <- as.numeric(aa$skewness_roll_forearm)
aa$skewness_pitch_forearm <- as.numeric(aa$skewness_pitch_forearm)
aa$max_yaw_forearm <- as.numeric(aa$max_yaw_forearm)
aa$min_yaw_forearm <- as.numeric(aa$min_yaw_forearm)

testing$kurtosis_roll_belt  <- as.numeric(testing$kurtosis_roll_belt)
testing$kurtosis_picth_belt <- as.numeric(testing$kurtosis_picth_belt)
testing$skewness_roll_belt <- as.numeric(testing$skewness_roll_belt)
testing$skewness_roll_belt.1 <- as.numeric(testing$skewness_roll_belt.1)
testing$max_yaw_belt <- as.numeric(testing$max_yaw_belt)
testing$min_yaw_belt <- as.numeric(testing$min_yaw_belt)
testing$kurtosis_roll_arm <- as.numeric(testing$kurtosis_roll_arm)
testing$kurtosis_picth_arm <- as.numeric(testing$kurtosis_picth_arm)
testing$kurtosis_yaw_arm <- as.numeric(testing$kurtosis_yaw_arm)
testing$skewness_roll_arm <- as.numeric(testing$skewness_roll_arm)
testing$skewness_pitch_arm <- as.numeric(testing$skewness_pitch_arm)
testing$skewness_yaw_arm <- as.numeric(testing$skewness_yaw_arm)
testing$kurtosis_roll_dumbbell <- as.numeric(testing$kurtosis_roll_dumbbell)
testing$kurtosis_picth_dumbbell <- as.numeric(testing$kurtosis_picth_dumbbell)
testing$skewness_roll_dumbbell <- as.numeric(testing$skewness_roll_dumbbell)
testing$skewness_pitch_dumbbell <- as.numeric(testing$skewness_pitch_dumbbell)
testing$max_yaw_dumbbell <- as.numeric(testing$max_yaw_dumbbell)
testing$min_yaw_dumbbell <- as.numeric(testing$min_yaw_dumbbell)
testing$total_accel_dumbbell <- as.numeric(testing$total_accel_dumbbell)
testing$kurtosis_roll_forearm <- as.numeric(testing$kurtosis_roll_forearm)
testing$kurtosis_picth_forearm <- as.numeric(testing$kurtosis_picth_forearm)
testing$skewness_roll_forearm <- as.numeric(testing$skewness_roll_forearm)
testing$skewness_pitch_forearm <- as.numeric(testing$skewness_pitch_forearm)
testing$max_yaw_forearm <- as.numeric(testing$max_yaw_forearm)
testing$min_yaw_forearm <- as.numeric(testing$min_yaw_forearm)
```

```{r ,cache=TRUE,echo=TRUE}
str(aa[,1:7])
```

among the first 7 variables are 
user_name: who did the exercise
timestamps
new_window: a boolean variable. "yes" if we are among the 406 obs described above
num_window

the last is
```{r,cache=TRUE}
str(aa[,84])
```
class: a factor variable taking values A,B,C,D,E which is the value we want to predict.

Now i will show you how easy it was to make a very simple model with 100% accuracy, and i insist its 100%, not 95%, not 99% not 99.5%, not 99.9%  !

Before using the brute force of the CPU, let's do some exploratory data analysis to find the variables with the most explanating power. It turns out, you need only ONE variable, and that variable is called...
num_window !

```{r,cache=TRUE}
answers = rep("A", 20) #initialize vector

testing_hack <- testing[7]
 index <- c(74, 431, 439, 194, 235, 504, 485, 440, 323, 664, 859, 461, 257, 408, 779, 302, 48, 361, 72, 255)

hack_table <- aa[,c(7,84)]
hack_table1 <- table(hack_table)
head(hack_table1)
```

yes to num_window correspond only one classe ! no even overlapping problem !
the testing data does provide a num_window and simply making a lookup for the num_window in the table will provide the correct classe of course !!! because the authors (or the teaching staff ?) didn't take the elementary precaution of using testing data OUTSIDE the training data.

```{r,cache=TRUE}
hack_table2 <- cbind(rownames(hack_table1),colnames(hack_table1[,apply(hack_table1,1,which.max)]))

answers <- hack_table2[match(index,hack_table2[,1]),2]
# print(answers) try it on your PC
```

I've got 20/20 predictions in a few minutes, before even running a single model...

Now, call me a cheater... well nobody said that you should submit the 20 values output by a ML model !
and definitely i can call my procedure a model ! I'm ROFL when i see people in the forum complaining they can't find the right values whereas they have a so-called model with 99.5% accuracy. 

I feel I could end the assignment here, job done, but I'll play ball, and try to think what can we do if we DO NOT use this num_window. Lets continue some exploratory data analysis.

```{r,cache=TRUE}
head(aa[,c(1:5),7])
```

Look at the timestamp variable together with the username and classe. If we browse through the data, we can see that the people who participated in the experience did the whole thing in a short time frame. its not like they went one day and did exercise A, and went back 1 day later to re-do it again. No, they did a set of exercise A for a short period of time, then exercise B, ... that makes the whole procedure questionable, really... even if any model was successful, it wouldn't prove it would work for even the *SAME* persons on another day (testing and validation test taken from the same data...) !!! 

Of course, using the timestamp or any of the header variables would be cheating as much as using just num_window.
As Tomek Pyda said on the forum 
https://class.coursera.org/predmachlearn-014/forum/thread?thread_id=129:
"Lets look at yaw_belt: [he plots yaw_belt vs num_window]

It gives you much information about the time. Looks like some curl lifts had different starting values than others. Other variables carry a lot of information about num_window too (see yaw_belt,magnet_dumbbell_y etc)

I think what we are so effectively predicting is: "to which particular exercise the data belongs to". And the classe as a result. Not how well somebody did it. That is why we can predict classes C and D."[end of Tomek's quote]

one can also suspect some of the instruments were not reset when participants changed and thats altering the data.
Unfortunately, by lack of time and information, I will ignore that. I will just have a didactic approach for the rest.

Ok, for this project, as I've started a bit late, I will just focus on the RCART model, a Random Forest may be more appropriate. I was afraid of performance problems for RF, and using CARET for the RCART was already quite slow. Actually, reading the forum afterwards, it looks like randomForest using the package randomForest is quite fast, but I'm running out of time and will leave it for another day. Anyway, the rest of the discussion is more about methodology and RCART is enough for discussion about Tree modelling.

Let's create as always some training sets and validation sets
we will use cross validation with K-folds (k=10) and 10 repeats for our CART models.

```{r,cache=TRUE,warning=FALSE}
library(caret)
inTrain <- createDataPartition(y=aa$classe,p=0.7,list=FALSE)

set1 <- aa[inTrain,]
training1 <- set1[,8:84]
set1v <- aa[-inTrain,]
valid1 <- set1v[,8:84]

FitControl <- trainControl(method="repeatedcv",
                           number=10,
                           repeats=10
                           )
```

the first reference model I will use is on the training1 set, and tuneLength=10 to run it in a reasonable time.

```{r,cache=TRUE,warning=FALSE}
library(caret)
modFit1 <- train(classe~.,method="rpart",data=training1,trControl=FitControl,tuneLength=10)
print(confusionMatrix(predict(modFit1,newdata=valid1),valid1$classe))
```

see ? without much effort and without any pre-processing, without even trying randomForest, just being dumb, we've already got an honourable 70% accuracy. We can see also some patterns: we have already a very high specificity for all classes A to E (95%), but the sensitivity to B, C, and D is quite lower than for A (sitting) and E (walking). As a result, trying to predict the testing set, and comparing to the answers, we get only 8/20. far from the "predicted"" 14/20 (70%). It seems natural that sitting being the most stable and E the most instable they get detected more easily than B (sitting down) C(standing) and D(standing up)

the model tree uses 12 variables. 

I wanted to check if those 12 variables are very random. I mean, if we take off those 12 variables and re-run a CART model, theres still more than 60 variables to play with, and can we build a model with similar accuracy ?
It turns out i've tried it, and for the same command line, the accuracy of such a model drops to 0.5643
(not enough space to publish all the results, try it on your PC)

```{r,cache=TRUE}
# library(caret)
# testing without 12 variables found in the model 1
# training2 <- training1[,-c(1,59,56,58,57,3,53,2,19,71,39,48)]
# modFit2 <- train(classe~.,method="rpart",data=training2,trControl=FitControl,tuneLength=10)
# accuracy drops to 0.5643 with modFit2
```

so here are those 12 variables (the number is the index they have in the training1 object)
roll_belt 1 <br>
pitch_forearm 59 <br>
magnet_dumbbell_y 56 <--[we'll talk a bit about that one later] <br>
roll_forearm 58 <br>
magnet_dumbbell_z 57 <br>
yaw_belt 3           <-- [remember what Tomek said about it] <br>
accel_dumbbell_y 53 <br>
pitch_belt 2 <br>
magnet_belt_z 19 <br>
accel_forearm_x 71 <br>
roll_dumbbell 39 <br>
total_accel_dumbbell 48 <br>

Let's put these variables aside for later and try to explain why they seem important.

Let's keep on and move on to tuneLength=30

```{r,cache=TRUE,warning=FALSE}
library(caret)
modFit1pro <- train(classe~.,method="rpart",data=training1,trControl=FitControl,tuneLength=30)
print(confusionMatrix(predict(modFit1pro,newdata=valid1),valid1$classe))
final_res <- predict(modFit1pro$finalModel,newdata=testing)
print(colnames(final_res[,apply(final_res,1,which.max)]))
```

accuracy is improved to over 80% (0.817). now only class B suffers from a somewhat lower sensitivity.
we got 16/20 out of the answers of the test, more in line with the (predicted) accuracy. 
B was wrongly predicted twice (once to A and once to C), E was predicted wrongly once to C, and B was wrongly predicted to A once. again B causes problem.

Let's look what the final Tree looks like
```{r,cache=TRUE}
# print(modFit1pro$finalModel) try it on your PC. not enough space here
```
if you get a headache when trying to read this, its normal !

So, we can't do a writeup without at least a nice plot so here it is, 

```{r, cache=TRUE,echo=FALSE,warning=FALSE}
library(rattle)
fancyRpartPlot(modFit1pro$finalModel)
```

We can see that the RED leaves corresponding to E on the right, and DARK GREEN leaves corresponding to A on the left are easily found. the Roll Belt detects very well if someone is walking. while the pitch arm and yaw belt are very useful to detect someone sitting.
for B,C,D its more difficult to interpret, there are more variables than in the system with accuracy 70. but we see again that the variables magnet_dumbbell (x,y,z), total accel_dumbbell, accel_dumbell_y and the rest of the earlier 12 variables used in the less complex system appear quite a lot in the tree




So more tuning and more CPU time, surely would get us an even better model ?

wait a minute. lets try something first. I did the following experiment. create a training set without info on pedro, and test it on the subset exclusively composed of pedro's data. (i tried with tunelength = 10 because of lack of cpu time.)

```{r, echo=FALSE}
#create training and valid sets without pedro
set1b <- set1[!(set1$user_name=="pedro"),]
training1b <- set1b[,8:84]
set1bv <- set1v[!(set1v$user_name=="pedro"),]
valid1b <- set1bv[,8:84]
set1bw <- set1v[(set1v$user_name=="pedro"),]
valid1bw <- set1bw[,8:84]
# testing without pedro
library(caret)
modFit1b <- train(classe~.,method="rpart",data=training1b,trControl=FitControl,tuneLength=10)
# print(confusionMatrix(predict(modFit1b,newdata=valid1bw),valid1bw$classe))

```
here's an extract of the result.

Confusion Matrix and Statistics

          Reference <br>
Prediction   A   B   C   D   E <br>
         A   0   0   0   0   0 <br>
         B 192 159 157 134  38 <br>
         C   0   0   0   0   0 <br>
         D   9   0   0   1   0 <br>
         E   0   0   0   0 130 <br>

Overall Statistics <br>
                                          
               Accuracy : 0.3537 <br>
               
The accuracy dropped by half from 0.704 to 0.354 !!! the model kept predicting always B !!
Such a result puts again into perspective these algos. Surely in the study we took 6 people, its because it was assumed it may be enough to predict do HAR on ANYBODY ? Looking at our procedure, we can highly suspect that if we took a testing set from a 7th person, the prediction of the model would look ugly. Remember i was already suspicious that even if the SAME person took the test again, the results would look much more ugly. Really theres 19622 obs, but theres only 6 persons doing 5 exercices, so in some sense, only 30 experiments.

Also, we should be suspicious because to have a model which gets "only" to 80% accuracy like I did its already impossible to make much sense of the tree result and values.

I tried fitting the same kind of CART trees to the subset of 406 observations. the accuracy was significantly below the models i showed here for the same complexity

Now I'll show something quite interesting that I found through data exploration.
```{r, cache=TRUE,echo=FALSE}
aaa <- aa[,-(3:6)]
ans3  <-aggregate(aaa[,!names(aaa) %in% c("user_name","classe")],by=list(user_name=aaa$user_name,classe=aaa$classe),FUN=range) 
print(ans3$magnet_dumbbell_y [1:12,])

```

This is an extract of the range (min and max) of this series for class A (lines 1 to 6) and B (lines 7 to 12), each with 6 observations for each participant in the experiment. notably one participant, PEDRO (lines 6 and 12)! yes ! the one who made a failure of the test above, now you can see why perhaps ? that important variable that we see on the tree, has NEGATIVE values for PEDRO. has he inverted the dumbbells ? did he do his exercise upside down ? theres some big outliers in this series for other participants as well. maybe thats also why my little experiment above with pedro went SOOOO bad ! ;-)


In a last effort before the deadline, i wanted to try randomForest as i took courage from the fact in the forum that it could be run in reasonable time after all.

Here are the astonishing results:

```{r, cache=FALSE,echo=FALSE}
library(randomForest)
library(caret)


set.seed(1234)

testingRF <- testing
uu <- is.na(apply(testingRF,2,min))
testingRF <- testingRF[,!uu]
testingRF <- testingRF[,-(1:7)]
testingnames <- colnames(testingRF)

uuu <- colnames(training1) %in% testingnames
uuu[77] <- TRUE
trainingRF <- training1[,uuu]

modRF <- randomForest(classe~.,data=trainingRF,importance=TRUE,proximity=TRUE)
print(confusionMatrix(predict(modRF,newdata=valid1),valid1$classe))

```

99.5% accuracy with default settings !

```{r, cache=TRUE,echo=FALSE}
# print(predict(modRF,newdata=testingRF)) try it at home, Coursera Honor Code: don't reveal answers !
```

it gets better, 20/20 on the testing set !

So what are the important variables ?

```{r, cache=TRUE,echo=FALSE}
imp <- sort(importance(modRF)[,7],decreasing=TRUE)
print(imp[1:12])
```

very interestingly, 10 out of the 12 most important variables of the RF model are the same than in the simpler CART model we documented above, who had only 69% accuracy.

So, is computer science so great these days that a total noob can just press a button and get a 99.5% accurate model ? (or rather, a computer can learn everything by itself very fast ? )

the answer to both questions, i fear, is YES. 

However, my hunch feeling is that we are overfitting data. I highly doubt the model works as it is with 99% accuracy ITRL (in the real life). The reason is , i think the protocol of the experience is bad, so that the data is not independent (always a key assumption in statistical modelling). Actually it is almost continuous as we have seen. There are also artefacts (pedro) in the data, which means the more complex models had a turnaround to get along with the fact that Pedro was upside down (?), electrods inverted or whatever...Its great to have an intelligent system able to recognize errors made by users, but this time it may also be an error by the searchers? have they noticed such things? was it voluntary from their side ?

As the data is continuous, theres in reality much less observations than 19,000. So in the validation set, theres *already* some of the data used in the training set, if not ALL of it ! and I've demonstrated that its the same for the testing set.

to test that: (sorry no time for that): take only 1 every 3 or 4 row for example and see the loss of accuracy...

I'd be more impressed if the RF worked with a sample of 10,000 people or 1,000 people doing 10 times the exercices at different times with data taken only every second...and I'm sure the methods used here would work fine.

Also, a truly intelligent system would not look just at a photo of instant values in the instruments, but would look at how they evolve through time. thats not the approach taken by the searchers there with a testing set being instant values.


